{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ 6: Ансамбль моделей машинного обучения для прогнозирования криптовалют\n",
    "\n",
    "В этом ноутбуке мы создадим ансамбль моделей машинного обучения для прогнозирования движения цен криптовалют. Мы будем использовать:\n",
    "- Различные классические модели (XGBoost, Random Forest, Gradient Boosting)\n",
    "- Свёрточную нейронную сеть (CNN)\n",
    "- Стекинг для объединения предсказаний моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, Tuple\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from xgboost import XGBClassifier\n",
    "from assets.DataProvider import DataProvider\n",
    "from assets.FeaturesGenerator import FeaturesGenerator\n",
    "from assets.enums import DataPeriod, DataResolution\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Настройка окружения для воспроизводимости результатов\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "SEED = 777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение вспомогательных классов\n",
    "\n",
    "### CNN классификатор\n",
    "Реализуем свёрточную нейронную сеть, совместимую с интерфейсом sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier:\n",
    "    \"\"\"CNN классификатор, совместимый с интерфейсом sklearn\"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape: int = None, random_state: int = None,\n",
    "                 epochs: int = 10, batch_size: int = 32,\n",
    "                 conv1_filters: int = 32, conv2_filters: int = 64,\n",
    "                 dense_units: int = 64, dropout_rate: float = 0.5):\n",
    "        self.input_shape = input_shape\n",
    "        self.random_state = random_state\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.conv1_filters = conv1_filters\n",
    "        self.conv2_filters = conv2_filters\n",
    "        self.dense_units = dense_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.classes_ = None\n",
    "        self.model = None\n",
    "        \n",
    "    def get_params(self, deep: bool = True) -> dict:\n",
    "        return {\n",
    "            'input_shape': self.input_shape,\n",
    "            'random_state': self.random_state,\n",
    "            'epochs': self.epochs,\n",
    "            'batch_size': self.batch_size,\n",
    "            'conv1_filters': self.conv1_filters,\n",
    "            'conv2_filters': self.conv2_filters,\n",
    "            'dense_units': self.dense_units,\n",
    "            'dropout_rate': self.dropout_rate\n",
    "        }\n",
    "    \n",
    "    def set_params(self, **params) -> 'CNNClassifier':\n",
    "        for key, value in params.items():\n",
    "            if not hasattr(self, key):\n",
    "                raise ValueError(f'Invalid parameter {key} for estimator {self.__class__.__name__}')\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "    \n",
    "    def _build_model(self):\n",
    "        if self.random_state is not None:\n",
    "            tf.random.set_seed(self.random_state)\n",
    "            \n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Reshape((self.input_shape, 1), input_shape=(self.input_shape,)),\n",
    "            tf.keras.layers.Conv1D(self.conv1_filters, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling1D(2),\n",
    "            tf.keras.layers.Conv1D(self.conv2_filters, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling1D(2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(self.dense_units, activation='relu'),\n",
    "            tf.keras.layers.Dropout(self.dropout_rate),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @property\n",
    "    def _estimator_type(self):\n",
    "        return 'classifier'\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Обучение модели\"\"\"\n",
    "        if self.input_shape is None:\n",
    "            self.input_shape = X.shape[1]\n",
    "            \n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        if self.model is None:\n",
    "            self.model = self._build_model()\n",
    "            \n",
    "        self.model.fit(\n",
    "            X, y,\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Предсказание вероятностей классов\"\"\"\n",
    "        proba = self.model.predict(X, verbose=0)\n",
    "        return np.hstack([1 - proba, proba])\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказание классов\"\"\"\n",
    "        return (self.predict_proba(X)[:, 1] >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс для оценки моделей\n",
    "Создадим класс для вычисления метрик и визуализации результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    \"\"\"Класс для оценки и сравнения производительности моделей\"\"\"\n",
    "    \n",
    "    def evaluate_model(self, model, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Расчет различных метрик для оценки модели\"\"\"\n",
    "        y_pred = model.predict(X_test)\n",
    "        return {\n",
    "            'Accuracy': accuracy_score(y_test, y_pred),\n",
    "            'Precision': precision_score(y_test, y_pred),\n",
    "            'Recall': recall_score(y_test, y_pred),\n",
    "            'F1': f1_score(y_test, y_pred)\n",
    "        }\n",
    "    \n",
    "    def plot_model_comparison(self, model_metrics: Dict[str, Dict[str, float]]):\n",
    "        \"\"\"Построение графика сравнения производительности моделей\"\"\"\n",
    "        metrics = list(next(iter(model_metrics.values())).keys())\n",
    "        models = list(model_metrics.keys())\n",
    "        n_metrics = len(metrics)\n",
    "        n_models = len(models)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        width = 0.8 / n_models\n",
    "        \n",
    "        for i, model in enumerate(models):\n",
    "            x = np.arange(n_metrics)\n",
    "            values = [model_metrics[model][metric] for metric in metrics]\n",
    "            ax.bar(x + i * width, values, width, label=model)\n",
    "        \n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title('Сравнение производительности моделей')\n",
    "        ax.set_xticks(x + width * (len(models) - 1) / 2)\n",
    "        ax.set_xticklabels(metrics)\n",
    "        ax.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных\n",
    "\n",
    "Загрузим данные криптовалют и подготовим признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42) -> None:\n",
    "    \"\"\"Установка всех генераторов случайных чисел для воспроизводимости\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    print(f\"Используется seed: {seed}\")\n",
    "\n",
    "# Устанавливаем seed\n",
    "seed_everything(SEED)\n",
    "\n",
    "# Инициализируем провайдер данных\n",
    "data_provider = DataProvider(\n",
    "    tickers=['BTC/USDT'],  # Используем пару BTC/USDT\n",
    "    resolution=DataResolution.DAY_01,  # Дневные свечи\n",
    "    period=DataPeriod.YEAR_01  # Данные за год\n",
    ")\n",
    "\n",
    "# Загружаем и обрабатываем данные\n",
    "data_provider.data_load()\n",
    "\n",
    "# Получаем обработанные данные\n",
    "df = data_provider.data_processed['BTC/USDT']\n",
    "\n",
    "# Создаем целевую переменную (направление движения цены)\n",
    "y = (df['Returns'].shift(-1) > 0).astype(int)\n",
    "\n",
    "# Генерируем признаки\n",
    "features_generator = FeaturesGenerator()\n",
    "features, feature_names = features_generator.prepare_features(df)\n",
    "\n",
    "# Выравниваем данные по индексу\n",
    "y = y[features.index]\n",
    "\n",
    "# Конвертируем в numpy массивы\n",
    "X = features.values\n",
    "y = y.values\n",
    "\n",
    "# Удаляем последнюю строку, так как для неё нет метки\n",
    "X = X[:-1]\n",
    "y = y[:-1]\n",
    "\n",
    "# Масштабируем признаки\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Дополнительное масштабирование для лучшей обработки выбросов\n",
    "robust_scaler = RobustScaler()\n",
    "X_robust = robust_scaler.fit_transform(X_scaled)\n",
    "\n",
    "# Выводим размерности данных\n",
    "print(f\"Размерность X: {X_scaled.shape}, размерность y: {y.shape}\")\n",
    "\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.15, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание и обучение моделей\n",
    "\n",
    "Создадим оптимизированные базовые модели и ансамбль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем оптимизированные базовые модели\n",
    "base_models = {\n",
    "    'XGBoost': XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=SEED\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        class_weight='balanced',\n",
    "        bootstrap=True,\n",
    "        random_state=SEED\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        random_state=SEED\n",
    "    ),\n",
    "    'CNN': CNNClassifier(\n",
    "        input_shape=X_robust.shape[1],\n",
    "        random_state=SEED,\n",
    "        conv1_filters=128,\n",
    "        conv2_filters=256,\n",
    "        dense_units=128,\n",
    "        dropout_rate=0.4\n",
    "    )\n",
    "}\n",
    "\n",
    "# Создаем стекинг-ансамбль с оптимизированным мета-классификатором\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', base_models['XGBoost']),\n",
    "        ('rf', base_models['Random Forest']),\n",
    "        ('gb', base_models['Gradient Boosting']),\n",
    "        ('cnn', base_models['CNN'])\n",
    "    ],\n",
    "    final_estimator=XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        random_state=SEED\n",
    "    ),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение и оценка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем словарь для метрик и оценщик\n",
    "model_metrics = {}\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Обучаем и оцениваем базовые модели\n",
    "print(\"Обучение базовых моделей...\")\n",
    "for name, model in base_models.items():\n",
    "    print(f\"Обучение {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = evaluator.evaluate_model(model, X_test, y_test)\n",
    "    model_metrics[name] = metrics\n",
    "    print(f\"Производительность {name}: {metrics}\")\n",
    "\n",
    "# Обучаем и оцениваем стекинг-ансамбль\n",
    "print(\"\\nОбучение стекинг-ансамбля...\")\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "model_metrics['Stacking Ensemble'] = evaluator.evaluate_model(stacking_clf, X_test, y_test)\n",
    "print(f\"Производительность стекинг-ансамбля: {model_metrics['Stacking Ensemble']}\")\n",
    "\n",
    "# Выводим метрики\n",
    "print(\"\\nМетрики производительности моделей:\")\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "# Строим график сравнения\n",
    "evaluator.plot_model_comparison(model_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
